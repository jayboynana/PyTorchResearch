{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decd2a3b-8cae-4d01-9c8e-c8d7fbc8f494",
   "metadata": {},
   "source": [
    "## TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f99d10-34fa-457b-8e83-d43305210ae0",
   "metadata": {},
   "source": [
    "### finetuning a pre-trained Mask R-CNN model on the Penn-Fudan Database for Pedestrian Detection and Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90655c-da72-44ac-9fff-866f4074094e",
   "metadata": {},
   "source": [
    "#### Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c86c4f-05a8-4279-8f90-c280f3d65e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15309b7-4d1e-4708-8abd-d65756c52cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PennFudanDataset(Dataset):\n",
    "    def __init__(self,root,transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root,\"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root,\"PedMasks\"))))\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        img_path = os.path.join(self.root,\"PNGImages\",self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root,\"PedMasks\",self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        obj_ids = torch.unique(mask)\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "        \n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "        boxes = masks_to_boxes(masks)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        image_id = torch.as_tensor(idx)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "106c9409-cb21-42f5-8a9c-a09d6f36b08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = './PennFudanPed/PennFudanPed/PNGImages/FudanPed00001.png'\n",
    "mask_path = './PennFudanPed/PennFudanPed/PedMasks/FudanPed00001_mask.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22460994-b1dd-44ef-b2e1-f6664c62fb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = read_image(image_path)\n",
    "mask = read_image(mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a9aac9-9a74-409f-b236-201448f2167a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[211, 210, 212,  ..., 143, 146, 148],\n",
       "         [179, 173, 170,  ..., 126, 128, 128],\n",
       "         [204, 194, 188,  ..., 130, 131, 130],\n",
       "         ...,\n",
       "         [226, 217, 211,  ..., 183, 184, 185],\n",
       "         [231, 219, 210,  ..., 187, 187, 186],\n",
       "         [225, 227, 215,  ..., 190, 190, 187]],\n",
       "\n",
       "        [[200, 199, 201,  ...,  96,  99, 101],\n",
       "         [168, 162, 159,  ...,  79,  81,  81],\n",
       "         [193, 183, 177,  ...,  83,  84,  83],\n",
       "         ...,\n",
       "         [220, 211, 205,  ..., 183, 184, 185],\n",
       "         [225, 213, 204,  ..., 187, 187, 186],\n",
       "         [219, 221, 209,  ..., 190, 190, 187]],\n",
       "\n",
       "        [[182, 181, 183,  ...,  78,  81,  83],\n",
       "         [150, 144, 141,  ...,  61,  63,  63],\n",
       "         [175, 165, 159,  ...,  65,  66,  65],\n",
       "         ...,\n",
       "         [220, 211, 205,  ..., 183, 184, 185],\n",
       "         [225, 213, 204,  ..., 187, 187, 186],\n",
       "         [219, 221, 209,  ..., 190, 190, 187]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "541c464e-76a3-47c4-bac0-17974977c988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b93fc18e-b7bb-41e2-9f53-a3abe78f7ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 536, 559])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce5cf5ab-3efc-4df3-b9f6-b51e67899b34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_ids = torch.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ffa17c7-5461-4406-96d3-b385ebc544cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0]],\n",
       "\n",
       "        [[1]],\n",
       "\n",
       "        [[2]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_ids[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddf42857-6bb3-4048-b2c9-7ea760d95ca3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True],\n",
       "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask == obj_ids[:, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "273ef473-eca2-412a-9d5d-625eeca14eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "masks = (mask == obj_ids[:, None, None]).to(dtype = torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dcc4c0c-f45d-47b4-be2f-bd9c542988a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6f9ff72-b632-444d-beef-2a4d8daf8cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boxes = masks_to_boxes(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37e3db44-2842-457d-9050-36f7bcaec2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0., 558., 535.],\n",
       "        [159., 181., 301., 430.],\n",
       "        [419., 170., 534., 485.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2da481f9-8e1c-4afa-900a-db04b5e60369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = './PennFudanPed/PennFudanPed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "304e377c-58c7-4c72-a217-0c9488a61005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PennFudanDataset = PennFudanDataset(root,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ead7a5fc-0a3d-4649-bfac-b1d77b665ad1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FudanPed00001_mask.png',\n",
       " 'FudanPed00002_mask.png',\n",
       " 'FudanPed00003_mask.png',\n",
       " 'FudanPed00004_mask.png',\n",
       " 'FudanPed00005_mask.png',\n",
       " 'FudanPed00006_mask.png',\n",
       " 'FudanPed00007_mask.png',\n",
       " 'FudanPed00008_mask.png',\n",
       " 'FudanPed00009_mask.png',\n",
       " 'FudanPed00010_mask.png',\n",
       " 'FudanPed00011_mask.png',\n",
       " 'FudanPed00012_mask.png',\n",
       " 'FudanPed00013_mask.png',\n",
       " 'FudanPed00014_mask.png',\n",
       " 'FudanPed00015_mask.png',\n",
       " 'FudanPed00016_mask.png',\n",
       " 'FudanPed00017_mask.png',\n",
       " 'FudanPed00018_mask.png',\n",
       " 'FudanPed00019_mask.png',\n",
       " 'FudanPed00020_mask.png',\n",
       " 'FudanPed00021_mask.png',\n",
       " 'FudanPed00022_mask.png',\n",
       " 'FudanPed00023_mask.png',\n",
       " 'FudanPed00024_mask.png',\n",
       " 'FudanPed00025_mask.png',\n",
       " 'FudanPed00026_mask.png',\n",
       " 'FudanPed00027_mask.png',\n",
       " 'FudanPed00028_mask.png',\n",
       " 'FudanPed00029_mask.png',\n",
       " 'FudanPed00030_mask.png',\n",
       " 'FudanPed00031_mask.png',\n",
       " 'FudanPed00032_mask.png',\n",
       " 'FudanPed00033_mask.png',\n",
       " 'FudanPed00034_mask.png',\n",
       " 'FudanPed00035_mask.png',\n",
       " 'FudanPed00036_mask.png',\n",
       " 'FudanPed00037_mask.png',\n",
       " 'FudanPed00038_mask.png',\n",
       " 'FudanPed00039_mask.png',\n",
       " 'FudanPed00040_mask.png',\n",
       " 'FudanPed00041_mask.png',\n",
       " 'FudanPed00042_mask.png',\n",
       " 'FudanPed00043_mask.png',\n",
       " 'FudanPed00044_mask.png',\n",
       " 'FudanPed00045_mask.png',\n",
       " 'FudanPed00046_mask.png',\n",
       " 'FudanPed00047_mask.png',\n",
       " 'FudanPed00048_mask.png',\n",
       " 'FudanPed00049_mask.png',\n",
       " 'FudanPed00050_mask.png',\n",
       " 'FudanPed00051_mask.png',\n",
       " 'FudanPed00052_mask.png',\n",
       " 'FudanPed00053_mask.png',\n",
       " 'FudanPed00054_mask.png',\n",
       " 'FudanPed00055_mask.png',\n",
       " 'FudanPed00056_mask.png',\n",
       " 'FudanPed00057_mask.png',\n",
       " 'FudanPed00058_mask.png',\n",
       " 'FudanPed00059_mask.png',\n",
       " 'FudanPed00060_mask.png',\n",
       " 'FudanPed00061_mask.png',\n",
       " 'FudanPed00062_mask.png',\n",
       " 'FudanPed00063_mask.png',\n",
       " 'FudanPed00064_mask.png',\n",
       " 'FudanPed00065_mask.png',\n",
       " 'FudanPed00066_mask.png',\n",
       " 'FudanPed00067_mask.png',\n",
       " 'FudanPed00068_mask.png',\n",
       " 'FudanPed00069_mask.png',\n",
       " 'FudanPed00070_mask.png',\n",
       " 'FudanPed00071_mask.png',\n",
       " 'FudanPed00072_mask.png',\n",
       " 'FudanPed00073_mask.png',\n",
       " 'FudanPed00074_mask.png',\n",
       " 'PennPed00001_mask.png',\n",
       " 'PennPed00002_mask.png',\n",
       " 'PennPed00003_mask.png',\n",
       " 'PennPed00004_mask.png',\n",
       " 'PennPed00005_mask.png',\n",
       " 'PennPed00006_mask.png',\n",
       " 'PennPed00007_mask.png',\n",
       " 'PennPed00008_mask.png',\n",
       " 'PennPed00009_mask.png',\n",
       " 'PennPed00010_mask.png',\n",
       " 'PennPed00011_mask.png',\n",
       " 'PennPed00012_mask.png',\n",
       " 'PennPed00013_mask.png',\n",
       " 'PennPed00014_mask.png',\n",
       " 'PennPed00015_mask.png',\n",
       " 'PennPed00016_mask.png',\n",
       " 'PennPed00017_mask.png',\n",
       " 'PennPed00018_mask.png',\n",
       " 'PennPed00019_mask.png',\n",
       " 'PennPed00020_mask.png',\n",
       " 'PennPed00021_mask.png',\n",
       " 'PennPed00022_mask.png',\n",
       " 'PennPed00023_mask.png',\n",
       " 'PennPed00024_mask.png',\n",
       " 'PennPed00025_mask.png',\n",
       " 'PennPed00026_mask.png',\n",
       " 'PennPed00027_mask.png',\n",
       " 'PennPed00028_mask.png',\n",
       " 'PennPed00029_mask.png',\n",
       " 'PennPed00030_mask.png',\n",
       " 'PennPed00031_mask.png',\n",
       " 'PennPed00032_mask.png',\n",
       " 'PennPed00033_mask.png',\n",
       " 'PennPed00034_mask.png',\n",
       " 'PennPed00035_mask.png',\n",
       " 'PennPed00036_mask.png',\n",
       " 'PennPed00037_mask.png',\n",
       " 'PennPed00038_mask.png',\n",
       " 'PennPed00039_mask.png',\n",
       " 'PennPed00040_mask.png',\n",
       " 'PennPed00041_mask.png',\n",
       " 'PennPed00042_mask.png',\n",
       " 'PennPed00043_mask.png',\n",
       " 'PennPed00044_mask.png',\n",
       " 'PennPed00045_mask.png',\n",
       " 'PennPed00046_mask.png',\n",
       " 'PennPed00047_mask.png',\n",
       " 'PennPed00048_mask.png',\n",
       " 'PennPed00049_mask.png',\n",
       " 'PennPed00050_mask.png',\n",
       " 'PennPed00051_mask.png',\n",
       " 'PennPed00052_mask.png',\n",
       " 'PennPed00053_mask.png',\n",
       " 'PennPed00054_mask.png',\n",
       " 'PennPed00055_mask.png',\n",
       " 'PennPed00056_mask.png',\n",
       " 'PennPed00057_mask.png',\n",
       " 'PennPed00058_mask.png',\n",
       " 'PennPed00059_mask.png',\n",
       " 'PennPed00060_mask.png',\n",
       " 'PennPed00061_mask.png',\n",
       " 'PennPed00062_mask.png',\n",
       " 'PennPed00063_mask.png',\n",
       " 'PennPed00064_mask.png',\n",
       " 'PennPed00065_mask.png',\n",
       " 'PennPed00066_mask.png',\n",
       " 'PennPed00067_mask.png',\n",
       " 'PennPed00068_mask.png',\n",
       " 'PennPed00069_mask.png',\n",
       " 'PennPed00070_mask.png',\n",
       " 'PennPed00071_mask.png',\n",
       " 'PennPed00072_mask.png',\n",
       " 'PennPed00073_mask.png',\n",
       " 'PennPed00074_mask.png',\n",
       " 'PennPed00075_mask.png',\n",
       " 'PennPed00076_mask.png',\n",
       " 'PennPed00077_mask.png',\n",
       " 'PennPed00078_mask.png',\n",
       " 'PennPed00079_mask.png',\n",
       " 'PennPed00080_mask.png',\n",
       " 'PennPed00081_mask.png',\n",
       " 'PennPed00082_mask.png',\n",
       " 'PennPed00083_mask.png',\n",
       " 'PennPed00084_mask.png',\n",
       " 'PennPed00085_mask.png',\n",
       " 'PennPed00086_mask.png',\n",
       " 'PennPed00087_mask.png',\n",
       " 'PennPed00088_mask.png',\n",
       " 'PennPed00089_mask.png',\n",
       " 'PennPed00090_mask.png',\n",
       " 'PennPed00091_mask.png',\n",
       " 'PennPed00092_mask.png',\n",
       " 'PennPed00093_mask.png',\n",
       " 'PennPed00094_mask.png',\n",
       " 'PennPed00095_mask.png',\n",
       " 'PennPed00096_mask.png']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PennFudanDataset.masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efc567ff-1d21-4fdd-bef9-4b54509cf737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PennFudanDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce4fe2b3-b8b5-4769-9be7-cd565fe2a378",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[211, 210, 212,  ..., 143, 146, 148],\n",
       "          [179, 173, 170,  ..., 126, 128, 128],\n",
       "          [204, 194, 188,  ..., 130, 131, 130],\n",
       "          ...,\n",
       "          [226, 217, 211,  ..., 183, 184, 185],\n",
       "          [231, 219, 210,  ..., 187, 187, 186],\n",
       "          [225, 227, 215,  ..., 190, 190, 187]],\n",
       " \n",
       "         [[200, 199, 201,  ...,  96,  99, 101],\n",
       "          [168, 162, 159,  ...,  79,  81,  81],\n",
       "          [193, 183, 177,  ...,  83,  84,  83],\n",
       "          ...,\n",
       "          [220, 211, 205,  ..., 183, 184, 185],\n",
       "          [225, 213, 204,  ..., 187, 187, 186],\n",
       "          [219, 221, 209,  ..., 190, 190, 187]],\n",
       " \n",
       "         [[182, 181, 183,  ...,  78,  81,  83],\n",
       "          [150, 144, 141,  ...,  61,  63,  63],\n",
       "          [175, 165, 159,  ...,  65,  66,  65],\n",
       "          ...,\n",
       "          [220, 211, 205,  ..., 183, 184, 185],\n",
       "          [225, 213, 204,  ..., 187, 187, 186],\n",
       "          [219, 221, 209,  ..., 190, 190, 187]]], dtype=torch.uint8),\n",
       " {'boxes': tensor([[159., 181., 301., 430.],\n",
       "          [419., 170., 534., 485.]]),\n",
       "  'labels': tensor([1, 1]),\n",
       "  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]],\n",
       "  \n",
       "          [[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n",
       "  'image_id': tensor(0),\n",
       "  'area': tensor([35358., 36225.]),\n",
       "  'iscrowd': tensor([0, 0])})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PennFudanDataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78326f4-db06-48ac-960c-55e162318642",
   "metadata": {},
   "source": [
    "#### Defining model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50a5be-49c5-49d0-af71-ad6632bf4e90",
   "metadata": {},
   "source": [
    "##### There are two common situations where one might want to modify one of the available models in TorchVision Model Zoo. The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).\n",
    "##### 也就是有两种情况，一个是我们使用预训练的模型，只是微调最后一层分类的类别；另外一个是我们需要更换骨干网络（比如，为了更快的预测）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123b6b30-7de9-427c-9e4f-ce4c3c696a94",
   "metadata": {},
   "source": [
    "##### 1.Finetuning from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3391da4-8a08-40de-b7c0-13638ef1844b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086a87ea-b45a-452e-ade9-0473d38e631b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\59585/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e61f93f-8c00-4b5d-8b76-cc803b0a048c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e606cc5-8348-42b8-8dc8-79d28eb8dfbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f4706-a1ba-4e2d-9bb2-8250c4092992",
   "metadata": {},
   "source": [
    "##### 2.Modifying the model to add a different backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08c9bd65-8ce2-48ae-97d4-a06bd57f71b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ae690f-96e4-4eb7-abff-7cb66bc8e810",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to C:\\Users\\59585/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-7ebf99e0.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2380fdd-5ef4-449d-b3f2-d037a7234d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone.out_channels = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b92fefd-da09-4115-9dea-0eddf65ac31c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2c4836e-b87b-45f0-8e2b-36786554c6a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=['0'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca95d754-ae11-4b11-9139-a4f2b0527aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=2,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c53c5-8650-4d19-b2bd-9cdab021dffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "529ced6c-43e3-4124-b865-2937b021475b",
   "metadata": {},
   "source": [
    "#### Object detection and instance segmentation model for PennFudan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e128ab-fac4-424e-9b2c-bc4f2154e89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyTorch]",
   "language": "python",
   "name": "conda-env-PyTorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
